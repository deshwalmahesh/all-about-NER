{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwRuUfnRgNo8"
      },
      "source": [
        "# Install, Imports & `Init`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-sgUbzBXPZK",
        "outputId": "96ca9343-be26-4787-cb48-e062b353f05a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/225.3 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate accelerate seqeval sklearn-crfsuite mlflow ipywidgets tqdm -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "IEnlUbgm8z3B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import pipeline\n",
        "import evaluate\n",
        "from datasets import Dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import Perceptron, SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "import sklearn_crfsuite\n",
        "\n",
        "import random\n",
        "import os\n",
        "import mlflow\n",
        "\n",
        "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = \"NER Task\"\n",
        "os.environ[\"MLFLOW_FLATTEN_PARAMS\"] = \"1\"\n",
        "\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "label_list = ['O','B-corporation','I-corporation','B-creative-work','I-creative-work','B-group','I-group','B-location','I-location','B-person','I-person','B-product','I-product']\n",
        "\n",
        "label2id = dict(zip(label_list, range(len(label_list))))\n",
        "id2label = {v:k for k,v in label2id.items()}\n",
        "\n",
        "\n",
        "NUM_LABELS = len(label_list)\n",
        "MAX_LEN = 196\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 2e-05\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OALaKRTagKGD"
      },
      "source": [
        "# Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "-XUBPQ-XcqBJ"
      },
      "outputs": [],
      "source": [
        "def load_split_data(file, SEED, return_splits = True):\n",
        "    \"\"\"\n",
        "    Load the data from text file and return it in List of List format with train test split\n",
        "    \"\"\"\n",
        "    with open(file,\"r\") as f: raw_data = [x.strip().split(\"\\t\") for x in f.readlines()]\n",
        "\n",
        "    tweets_list = []\n",
        "    entities_list = []\n",
        "\n",
        "    temp_ent = []\n",
        "    temp_words = []\n",
        "\n",
        "    for index, lis in enumerate(raw_data):\n",
        "        try:\n",
        "            if lis == [\"\"]:\n",
        "                assert len(temp_words) == len(temp_ent), \"Sanity Check: Irregular Length\"\n",
        "                tweets_list.append(temp_words)\n",
        "                entities_list.append(temp_ent)\n",
        "\n",
        "                temp_words = []\n",
        "                temp_ent = []\n",
        "            else:\n",
        "                (word,entity) = lis\n",
        "\n",
        "                word = word.strip()\n",
        "                if not len(word): continue\n",
        "\n",
        "                entity = entity.strip()\n",
        "\n",
        "                temp_words.append(word)\n",
        "                temp_ent.append(entity)\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e, index, lis)\n",
        "\n",
        "    assert len(tweets_list) == len(entities_list), \"entity text length mismatch\"\n",
        "\n",
        "    if return_splits: return train_test_split(tweets_list, entities_list, test_size=0.2, random_state = SEED)\n",
        "    return tweets_list, entities_list\n",
        "\n",
        "\n",
        "def convert_label_to_int(entities_list):\n",
        "  \"\"\"\n",
        "  Convert [\"O\", \"I-PER\"...] to their respective ids\n",
        "  \"\"\"\n",
        "  return [[label2id[label] for label in label_list] for label_list in entities_list]\n",
        "\n",
        "\n",
        "def aligned_tokenization_for_NER(input_data, label_all_tokens = False):\n",
        "\n",
        "    tokenized_inputs = tokenizer(input_data[\"tokens\"], truncation = True, is_split_into_words = True, )#padding = 'max_length', max_length = MAX_LEN)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(input_data[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:  # Set the special tokens to -100 to be ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "\n",
        "            elif word_idx != previous_word_idx: # label ONLY for the first token of each word\n",
        "                label_ids.append(label[word_idx])\n",
        "\n",
        "            else: # For the other tokens in a word, set the label to either the current label or -100, depending on\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)]\n",
        "\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "    flattened_results = {\n",
        "        \"overall_precision\": results[\"overall_precision\"],\n",
        "        \"overall_recall\": results[\"overall_recall\"],\n",
        "        \"overall_f1\": results[\"overall_f1\"],\n",
        "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "    for k in results.keys(): # piece taken from https://www.freecodecamp.org/news/getting-started-with-ner-models-using-huggingface/\n",
        "      if(k not in flattened_results.keys()):\n",
        "        flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
        "\n",
        "    return flattened_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui_zN-0jgXFj"
      },
      "source": [
        "# Data Pre-Processing, Sanity checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "ErVt0WSEUS53"
      },
      "outputs": [],
      "source": [
        "train_tokens, val_tokens, train_ent, val_ent = load_split_data(\"./train.txt\", SEED)\n",
        "\n",
        "train_labels = convert_label_to_int(train_ent)\n",
        "val_labels = convert_label_to_int(val_ent)\n",
        "\n",
        "# Sanity Checking\n",
        "len(train_tokens) == len(train_labels) == len(train_ent), \"Sanity Check failed\"\n",
        "for i in range(len(train_tokens)):\n",
        "  assert len(train_tokens[i]) == len(train_labels[i]) == len(train_ent[i]), \"Sanity Check Failed\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "316pCFli9QCz"
      },
      "source": [
        "## [`sklearn` + `CRF`](https://github.com/TeamHG-Memex/sklearn-crfsuite) (Not maintained. Won't run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uuGjJQIN9TAO"
      },
      "outputs": [],
      "source": [
        "# def run_CRF(train_tokens, val_tokens, train_ent, val_ent, model):\n",
        "#   train_sentences = [list(zip(train_tokens[i],train_ent[i])) for i in range(len(train_tokens))]\n",
        "#   val_sentences = [list(zip(val_tokens[i],val_ent[i])) for i in range(len(val_tokens))]\n",
        "\n",
        "\n",
        "#   X_train = [sent2features(s) for s in train_sentences]\n",
        "#   y_train = [sent2labels(s) for s in train_sentences]\n",
        "\n",
        "#   X_val = [sent2features(s) for s in train_sentences]\n",
        "#   y_val = [sent2labels(s) for s in train_sentences]\n",
        "\n",
        "#   model.fit(X_train, y_train)\n",
        "#   preds = model.predict(X_val)\n",
        "\n",
        "#   classes = np.unique(y_train)\n",
        "#   classes = classes.tolist()\n",
        "#   new_classes = classes.copy()\n",
        "#   new_classes.pop() # remove \"O\" for better visibility\n",
        "\n",
        "#   print(classification_report(y_pred=preds, y_true=y_val, labels=new_classes), \"\\n\",\"-\"*50)\n",
        "#   return preds\n",
        "\n",
        "\n",
        "# def word2features(sent, i):\n",
        "#     word = sent[i][0]\n",
        "\n",
        "#     features = {\n",
        "#         'bias': 1.0,\n",
        "#         'word.lower()': word.lower(),\n",
        "#         'word[-3:]': word[-3:],\n",
        "#         'word[-2:]': word[-2:],\n",
        "#         'word.isupper()': word.isupper(),\n",
        "#         'word.istitle()': word.istitle(),\n",
        "#         'word.isdigit()': word.isdigit(),\n",
        "#     }\n",
        "#     if i > 0:\n",
        "#         word1 = sent[i-1][0]\n",
        "#         postag1 = sent[i-1][1]\n",
        "#         features.update({\n",
        "#             '-1:word.lower()': word1.lower(),\n",
        "#             '-1:word.istitle()': word1.istitle(),\n",
        "#             '-1:word.isupper()': word1.isupper(),\n",
        "#         })\n",
        "#     else:\n",
        "#         features['BOS'] = True\n",
        "#     if i < len(sent)-1:\n",
        "#         word1 = sent[i+1][0]\n",
        "#         postag1 = sent[i+1][1]\n",
        "#         features.update({\n",
        "#             '+1:word.lower()': word1.lower(),\n",
        "#             '+1:word.istitle()': word1.istitle(),\n",
        "#             '+1:word.isupper()': word1.isupper(),\n",
        "#         })\n",
        "#     else:\n",
        "#         features['EOS'] = True\n",
        "#     return features\n",
        "\n",
        "\n",
        "# def sent2features(sent):\n",
        "#     return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "# def sent2tokens(sent):\n",
        "#     return [item[0] for item in sent]\n",
        "\n",
        "# def sent2labels(sent):\n",
        "#     return [item[1] for item in sent]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "j5dvWP0nAzVi",
        "outputId": "533339c9-3955-4062-bd85-a0dce7c2bf7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/arraysetops.py:270: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  ar = np.asanyarray(ar)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-30bda2ed7f3c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn_crfsuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCRF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_possible_transitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_CRF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-1715e4989b34>\u001b[0m in \u001b[0;36mrun_CRF\u001b[0;34m(train_tokens, val_tokens, train_ent, val_ent, model)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mnew_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# remove \"O\" for better visibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     \"\"\"\n\u001b[1;32m   2309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2310\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[1;32m     86\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         ):\n\u001b[0;32m--> 345\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    346\u001b[0m                 \u001b[0;34m\"You appear to be using a legacy multi-label data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;34m\" representation. Sequence of sequences are no\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format."
          ]
        }
      ],
      "source": [
        "# model = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=20, all_possible_transitions = False)\n",
        "\n",
        "# preds = run_CRF(train_tokens, val_tokens, train_ent, val_ent, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2m3mUhq0STG"
      },
      "source": [
        "# Intial training attempt using [Bi-LSTM + CRF + Viterbi Decoding](https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html)\n",
        "\n",
        "Training the first Deep Learning model from scratch to check if there are any gains using [Bi-LSTM + CRF + Viterbi Decoding](https://jovian.com/abdulmajee/bilstm-crf)\n",
        "\n",
        "**TOO SLOW TO USE**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRqnpjX83bF8"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "CqqImbyM0dhI"
      },
      "outputs": [],
      "source": [
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, device):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.device = device\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        a = torch.randn(2, 1, self.hidden_dim // 2).to(self.device)\n",
        "        b = torch.randn(2, 1, self.hidden_dim // 2).to(self.device)\n",
        "        return (a,b)\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(self.device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(self.device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(self.device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(self.device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence).to(self.device)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVOwRgIH4XOG"
      },
      "source": [
        "## Data Formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "fCviv_Q94chs"
      },
      "outputs": [],
      "source": [
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 64\n",
        "BATCH = 64\n",
        "\n",
        "\n",
        "train_tokens, val_tokens, train_ent, val_ent\n",
        "\n",
        "training_data = list(zip(train_tokens, train_ent)) # in the format [([word1, word2],[tag1,tag2]),([word1,word2,word3],[tag1,tag2,tag3])]\n",
        "validation_data = list(zip(val_tokens, val_ent))\n",
        "\n",
        "word_to_ix = {}\n",
        "for sentence, tags in training_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "tag_to_ix = dict(zip(label_list, range(len(label_list))))\n",
        "tag_to_ix[START_TAG] = 13\n",
        "tag_to_ix[STOP_TAG] =  14\n",
        "\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, training_data):\n",
        "      self.length = 0\n",
        "      self.sentences = []\n",
        "      self.targets = []\n",
        "      for sentence, tags in training_data:\n",
        "        self.sentences.append(prepare_sequence(sentence, word_to_ix)) # Input to tensors\n",
        "        self.targets.append(torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long))\n",
        "\n",
        "        self.length += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx], self.targets[idx]\n",
        "\n",
        "\n",
        "# Batch training data\n",
        "t_d = CustomDataset(training_data)\n",
        "train_data_loader = DataLoader(t_d, batch_size=1, shuffle=True)\n",
        "# val_data_loader = CustomDataset(validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEHGKD1K3koY"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "-cSJ9kjy3fib",
        "outputId": "9ccf08b4-7555-4e20-d5e7-319966d572ef"
      },
      "outputs": [],
      "source": [
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, device = device)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4, weight_decay=1e-2)\n",
        "model = model.to(device)\n",
        "\n",
        "# INIT model once for loading everything beforehand\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
        "    print(model(precheck_sent.to(device)))\n",
        "\n",
        "\n",
        "for epoch in range(3): # epochs\n",
        "    for s, t in tqdm(train_data_loader):\n",
        "        model.zero_grad() # clear grds\n",
        "\n",
        "        loss = model.neg_log_likelihood(s[0].to(device), t[0].to(device)) # forward -> loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# # Check predictions after training\n",
        "# with torch.no_grad():\n",
        "#     precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "#     print(model(precheck_sent))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "IG8z2Kh09Kh2",
        "NSo2BrCbKEvT",
        "Gqyc_1DUzO0p",
        "28wrtT2nzUHw",
        "XV7abLE5zWak"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
